{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84894,"databundleVersionId":9709193,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/suehuynh/kaggle-s4e10-light-automl-loan-prediction?scriptVersionId=214228643\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"markdown","source":"## Load libraries","metadata":{}},{"cell_type":"code","source":"!pip install -U lightautoml[all]","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-10-30T16:45:42.54891Z","iopub.execute_input":"2024-10-30T16:45:42.549462Z","iopub.status.idle":"2024-10-30T16:49:02.466283Z","shell.execute_reply.started":"2024-10-30T16:45:42.549409Z","shell.execute_reply":"2024-10-30T16:49:02.464281Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightautoml.automl.presets.tabular_presets import TabularAutoML\nfrom lightautoml.tasks import Task\nimport torch\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:02.470533Z","iopub.execute_input":"2024-10-30T16:49:02.471174Z","iopub.status.idle":"2024-10-30T16:49:35.512698Z","shell.execute_reply.started":"2024-10-30T16:49:02.471086Z","shell.execute_reply":"2024-10-30T16:49:35.511605Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd              # For data manipulation and analysis\nimport numpy as np               # For numerical computing\nfrom datetime import datetime\nimport scipy.stats as stats      # For statistical analysis\nimport math\nimport matplotlib                # For plotting and visualization\nimport matplotlib.pyplot as plt  \nfrom pandas.plotting import parallel_coordinates\nimport seaborn as sns            # For statistical data visualization\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:35.513919Z","iopub.execute_input":"2024-10-30T16:49:35.514573Z","iopub.status.idle":"2024-10-30T16:49:35.522655Z","shell.execute_reply.started":"2024-10-30T16:49:35.514531Z","shell.execute_reply":"2024-10-30T16:49:35.521434Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For machine learning\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier, Pool\n\nfrom lightgbm import early_stopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, roc_auc_score,\n                             f1_score, confusion_matrix, classification_report)\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.compose import ColumnTransformer\nimport optuna","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:35.524544Z","iopub.execute_input":"2024-10-30T16:49:35.524988Z","iopub.status.idle":"2024-10-30T16:49:35.849008Z","shell.execute_reply.started":"2024-10-30T16:49:35.524942Z","shell.execute_reply":"2024-10-30T16:49:35.847984Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/playground-series-s4e10/train.csv',index_col=0)\ndf_test = pd.read_csv('/kaggle/input/playground-series-s4e10/test.csv', index_col=0)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:35.851831Z","iopub.execute_input":"2024-10-30T16:49:35.852235Z","iopub.status.idle":"2024-10-30T16:49:36.080327Z","shell.execute_reply.started":"2024-10-30T16:49:35.852191Z","shell.execute_reply":"2024-10-30T16:49:36.079227Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_cols = [col for col in df_test.columns if df_test[col].dtypes in ['int', 'float']]\ncat_cols = [col for col in df_test.columns if col not in num_cols]\nprint(num_cols, cat_cols)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:36.081589Z","iopub.execute_input":"2024-10-30T16:49:36.082009Z","iopub.status.idle":"2024-10-30T16:49:36.08933Z","shell.execute_reply.started":"2024-10-30T16:49:36.081888Z","shell.execute_reply":"2024-10-30T16:49:36.088111Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\n\n# def feature_engineer(df):\n#     # Replace extreme outliers values > 100 with the median\n#     median_age = df['person_age'].median()\n#     df['person_age'] = df['person_age'].apply(lambda x: median_age if x > 100 else x)\n    \n#     median_emp_length = df['person_emp_length'].median()\n#     df['person_emp_length'] = df['person_emp_length'].apply(lambda x: median_emp_length if x > 100 else x)\n\n#     # Convert 'cb_person_default_on_file' Y to 1, N to 0\n#     df['cb_person_default_on_file'] = df['cb_person_default_on_file'].apply(lambda x: 1 if x == 'Y' else 0)\n\n#     # Group 'loan_grade' and binary encoding\n#     df['loan_grade_grouped'] = df['loan_grade'].apply(lambda x: 'ABC' if x in ['A', 'B', 'C'] else 'D_and_others')\n#     df = pd.get_dummies(df, columns=['loan_grade_grouped'], drop_first=True)\n#     df.drop('loan_grade', axis = 1, inplace = True)\n\n#     # One-hot encode 'loan_intent' and 'person_home_ownership'\n#     df = pd.get_dummies(df, columns=['loan_intent', 'person_home_ownership'], drop_first=True)\n\n#     # Convert boolean columns to 0 and 1\n#     bool_cols = df.select_dtypes(include='bool').columns\n#     df[bool_cols] = df[bool_cols].astype(int)\n    \n#     return df","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:36.09112Z","iopub.execute_input":"2024-10-30T16:49:36.092052Z","iopub.status.idle":"2024-10-30T16:49:36.101801Z","shell.execute_reply.started":"2024-10-30T16:49:36.092005Z","shell.execute_reply":"2024-10-30T16:49:36.100664Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def feature_engineer(df):\n    # 1. Age Group\n    df['age_group'] = pd.cut(df['person_age'], bins=[0, 24, 34, 44, 54, 100], \n                             labels=['<25', '25-34', '35-44', '45-54', '55+'])\n\n    # 2. Income Level\n    df['income_level'] = pd.cut(df['person_income'], bins=[0, 30000, 60000, 100000, float('inf')], \n                                labels=['Low', 'Medium', 'High', 'Very High'])\n\n    # 3. Employment Stability\n    df['employment_stability'] = pd.cut(df['person_emp_length'], bins=[0, 1, 5, 10, float('inf')],\n                                        labels=['<1 year', '1-5 years', '6-10 years', '10+ years'])\n\n    # 4. Loan Amount Range\n    df['loan_amnt_range'] = pd.cut(df['loan_amnt'], bins=[0, 5000, 10000, 20000, float('inf')],\n                                   labels=['<5k', '5-10k', '10-20k', '20k+'])\n\n    # 5. Interest Rate Buckets\n    df['loan_int_rate_range'] = pd.cut(df['loan_int_rate'], bins=[0, 5, 10, 15, 20, float('inf')],\n                                       labels=['<5%', '5-10%', '10-15%', '15-20%', '20%+'])\n\n    # 6. Loan-to-Income Ratio Buckets\n    df['loan_percent_income_range'] = pd.cut(df['loan_percent_income'], bins=[0, 0.2, 0.4, 0.6, float('inf')],\n                                             labels=['<20%', '20-40%', '40-60%', '60%+'])\n\n    # 7. Encode Default History\n    df['default_history'] = df['cb_person_default_on_file'].map({'N': 0, 'Y': 1})\n\n    # 8. Credit History Length Category\n    df['credit_hist_length_cat'] = pd.cut(df['cb_person_cred_hist_length'], bins=[0, 5, 10, 15, float('inf')],\n                                          labels=['<5 years', '5-10 years', '10-15 years', '15+ years'])\n\n    # 9. Interaction Feature: Income Level & Employment Stability\n    df['income_emp_interaction'] = df['income_level'].astype(str) + '_' + df['employment_stability'].astype(str)\n\n    # 10. Interaction Feature: Loan Grade & Interest Rate\n    df['grade_int_rate'] = df['loan_grade'] + '_' + df['loan_int_rate_range'].astype(str)\n\n    # 11. Debt Burden (Loan Amount Relative to Income)\n    df['debt_burden'] = df['loan_amnt'] / df['person_income']\n\n    # 12. Credit History per Age\n    df['credit_hist_per_age'] = df['cb_person_cred_hist_length'] / df['person_age']\n\n    # 13. Risk Score (Combining multiple risk factors)\n    df['risk_score'] = (\n        df['loan_int_rate'] * df['loan_percent_income'] / (df['cb_person_cred_hist_length'] + 1)\n    )\n\n    # 14. Aggregated Default Risk\n    df['high_risk_flag'] = ((df['default_history'] == 1) | \n                            (df['loan_int_rate'] > 15) |\n                            (df['loan_percent_income'] > 0.5)).astype(int)\n\n    # 15. Age-Adjusted Employment Length\n    df['emp_length_age_ratio'] = df['person_emp_length'] / df['person_age']\n\n    # 17. Loan Amount per Year of Employment\n    df['loan_per_year_emp'] = df['loan_amnt'] / (df['person_emp_length'] + 1)\n\n    # 18. Age-to-Income Ratio\n    df['age_to_income_ratio'] = df['person_age'] / df['person_income']\n\n    # 19. Adjusted Interest Rate (Interest Rate adjusted by Loan Grade)\n    loan_grade_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n    df['loan_grade_numeric'] = df['loan_grade'].map(loan_grade_map)\n    df['adjusted_interest_rate'] = df['loan_int_rate'] / (df['loan_grade_numeric'] + 1)\n\n    # 20. Credit Age Relative to Employment Length\n    df['credit_to_emp_ratio'] = df['cb_person_cred_hist_length'] / (df['person_emp_length'] + 1)\n\n    # 21. Financial Stability Index (FSI)\n    # A composite score combining income, loan amount, and interest rate to assess financial stability\n    df['financial_stability_index'] = df['person_income'] / (df['loan_amnt'] * df['loan_int_rate'])\n\n    # 22. Loan Amortization Speed (Hypothetical)\n    # The ratio of income to loan amount, indicating how quickly the loan might be paid off\n    df['loan_amortization_speed'] = df['person_income'] / df['loan_amnt']\n\n    # 23. Loan Purpose Risk Weight (Assign risk score based on loan intent)\n    loan_intent_risk_map = {'EDUCATION': 0.5, 'MEDICAL': 0.8, 'PERSONAL': 1.0, 'HOMEIMPROVEMENT': 0.6, \n                            'VENTURE': 1.5, 'DEBTCONSOLIDATION': 0.7}\n    df['loan_intent_risk'] = df['loan_intent'].map(loan_intent_risk_map)\n\n    # 24. Employment to Income Ratio\n    df['emp_income_ratio'] = df['person_emp_length'] / (df['person_income'] / 1000)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:36.103419Z","iopub.execute_input":"2024-10-30T16:49:36.103739Z","iopub.status.idle":"2024-10-30T16:49:36.124387Z","shell.execute_reply.started":"2024-10-30T16:49:36.103707Z","shell.execute_reply":"2024-10-30T16:49:36.12324Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_processed = feature_engineer(df_train)\ndf_test_processed = feature_engineer(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:36.126088Z","iopub.execute_input":"2024-10-30T16:49:36.126525Z","iopub.status.idle":"2024-10-30T16:49:36.39696Z","shell.execute_reply.started":"2024-10-30T16:49:36.126486Z","shell.execute_reply":"2024-10-30T16:49:36.395804Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_processed.info()","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:36.398646Z","iopub.execute_input":"2024-10-30T16:49:36.399115Z","iopub.status.idle":"2024-10-30T16:49:36.418412Z","shell.execute_reply.started":"2024-10-30T16:49:36.399068Z","shell.execute_reply":"2024-10-30T16:49:36.417251Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test_processed.info()","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:36.419709Z","iopub.execute_input":"2024-10-30T16:49:36.420039Z","iopub.status.idle":"2024-10-30T16:49:36.435193Z","shell.execute_reply.started":"2024-10-30T16:49:36.419996Z","shell.execute_reply":"2024-10-30T16:49:36.434027Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"num_cols = [col for col in df_test_processed.columns if df_test_processed[col].dtypes in ['int', 'float']]\ncat_cols = [col for col in df_test_processed.columns if col not in num_cols]\nprint(num_cols, cat_cols)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:36.436615Z","iopub.execute_input":"2024-10-30T16:49:36.436955Z","iopub.status.idle":"2024-10-30T16:49:36.443045Z","shell.execute_reply.started":"2024-10-30T16:49:36.436919Z","shell.execute_reply":"2024-10-30T16:49:36.442009Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Initialize the StandardScaler\nscaler = StandardScaler()\n\ndf_train_processed[num_cols] = scaler.fit_transform(df_train_processed[num_cols])\ndf_test_processed[num_cols] = scaler.transform(df_test_processed[num_cols])","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:36.44442Z","iopub.execute_input":"2024-10-30T16:49:36.444767Z","iopub.status.idle":"2024-10-30T16:49:36.496991Z","shell.execute_reply.started":"2024-10-30T16:49:36.444732Z","shell.execute_reply":"2024-10-30T16:49:36.496015Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_processed","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:36.501758Z","iopub.execute_input":"2024-10-30T16:49:36.502132Z","iopub.status.idle":"2024-10-30T16:49:36.5315Z","shell.execute_reply.started":"2024-10-30T16:49:36.502093Z","shell.execute_reply":"2024-10-30T16:49:36.530492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" \n    Iterate through all the columns of a dataframe and modify the data type\n    to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if pd.api.types.is_numeric_dtype(df[col]):\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if pd.api.types.is_integer_dtype(df[col]):\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                else:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('object')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:36.532743Z","iopub.execute_input":"2024-10-30T16:49:36.533062Z","iopub.status.idle":"2024-10-30T16:49:36.545037Z","shell.execute_reply.started":"2024-10-30T16:49:36.533022Z","shell.execute_reply":"2024-10-30T16:49:36.543865Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_processed = reduce_mem_usage(df_train_processed)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:36.546297Z","iopub.execute_input":"2024-10-30T16:49:36.546618Z","iopub.status.idle":"2024-10-30T16:49:36.580182Z","shell.execute_reply.started":"2024-10-30T16:49:36.546584Z","shell.execute_reply":"2024-10-30T16:49:36.579081Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test_processed = reduce_mem_usage(df_test_processed)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:36.581556Z","iopub.execute_input":"2024-10-30T16:49:36.581897Z","iopub.status.idle":"2024-10-30T16:49:36.607429Z","shell.execute_reply.started":"2024-10-30T16:49:36.581861Z","shell.execute_reply":"2024-10-30T16:49:36.606159Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Light AutoML","metadata":{}},{"cell_type":"code","source":"def map_class(x, task, reader):\n    if task.name == 'multiclass':\n        return reader[x]\n    else:\n        return x\n\nmapped = np.vectorize(map_class)\n\ndef score(task, y_true, y_pred):\n    if task.name == 'binary':\n        return roc_auc_score(y_true, y_pred)\n    elif task.name == 'multiclass':\n        return accuracy_score(y_true, np.argmax(y_pred, 1))\n    elif task.name == 'reg' or task.name == 'multi:reg':\n        return median_absolute_error(y_true, y_pred)\n    else:\n        raise 'Task is not correct.'\n        \ndef take_pred_from_task(pred, task):\n    if task.name == 'binary' or task.name == 'reg':\n        return pred[:, 0]\n    elif task.name == 'multiclass' or task.name == 'multi:reg':\n        return pred\n    else:\n        raise 'Task is not correct.'\n        \ndef use_plr(USE_PLR):\n    if USE_PLR:\n        return \"plr\"\n    else:\n        return \"cont\"","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:36.609071Z","iopub.execute_input":"2024-10-30T16:49:36.609489Z","iopub.status.idle":"2024-10-30T16:49:36.61844Z","shell.execute_reply.started":"2024-10-30T16:49:36.609449Z","shell.execute_reply":"2024-10-30T16:49:36.617235Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RANDOM_STATE = 42\nN_THREADS = os.cpu_count()\nnp.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:36.619932Z","iopub.execute_input":"2024-10-30T16:49:36.620415Z","iopub.status.idle":"2024-10-30T16:49:36.645331Z","shell.execute_reply.started":"2024-10-30T16:49:36.620365Z","shell.execute_reply":"2024-10-30T16:49:36.644402Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"task = Task('binary') \nautoml = TabularAutoML(\n    task = task, \n    timeout = 9 * 3600,\n    cpu_limit = os.cpu_count(),\n    nn_params = {\n        \"n_epochs\": 10, \n        \"bs\": 512, \n        \"num_workers\": 0, \n        \"path_to_save\": None, \n        \"freeze_defaults\": True,\n        \"cont_embedder\": 'plr',\n        'cat_embedder': 'weighted',\n        \"hidden_size\": 128,\n        'hid_factor': [4, 4],\n        'block_config': [4, 4],\n        'embedding_size': 128, \n        'stop_by_metric': True,\n        'verbose_bar': True,\n        \"snap_params\": { 'k': 2, 'early_stopping': True, 'patience': 2, 'swa': True}\n    },\n    nn_pipeline_params = {\"use_qnt\": False, \"use_te\": False},\n    reader_params = {'n_jobs': os.cpu_count(), 'cv': 10, 'random_state': 42, 'advanced_roles': True}\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:36.646633Z","iopub.execute_input":"2024-10-30T16:49:36.64764Z","iopub.status.idle":"2024-10-30T16:49:36.697479Z","shell.execute_reply.started":"2024-10-30T16:49:36.64759Z","shell.execute_reply":"2024-10-30T16:49:36.696365Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# task = Task('binary') \n# automl = TabularAutoML(\n#     task = task, \n#     timeout = 9 * 3600,\n#     cpu_limit = os.cpu_count(),\n#     nn_params = {\n#     'stop_by_metric': True,\n#         'verbose_bar': True},\n#     nn_pipeline_params = {\"use_qnt\": False, \"use_te\": False},\n#     reader_params = {'n_jobs': os.cpu_count(), 'cv': 10, 'random_state': 42, 'advanced_roles': True}\n# )","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:49:36.698825Z","iopub.execute_input":"2024-10-30T16:49:36.699213Z","iopub.status.idle":"2024-10-30T16:49:36.704296Z","shell.execute_reply.started":"2024-10-30T16:49:36.699162Z","shell.execute_reply":"2024-10-30T16:49:36.702982Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"out_of_fold_predictions = automl.fit_predict(\n    df_train_processed,\n    roles = {\n        'target': 'loan_status',\n    }, \n    verbose = 3\n)","metadata":{"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-10-30T16:49:36.706179Z","iopub.execute_input":"2024-10-30T16:49:36.706646Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Light AutoML ML Accuracy:\")\nroc_auc_score(df_train_processed.loan_status, out_of_fold_predictions.data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"X = df_train_processed.drop(columns = ['loan_status'] , axis = 1)\ny = df_train_processed['loan_status']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ndef prepare_data_for_xgb(df):\n    # Convert object columns to categorical if needed\n    object_cols = df.select_dtypes(include='object').columns\n    \n    # Use LabelEncoder or pd.get_dummies to encode categorical columns\n    label_encoder = LabelEncoder()\n    \n    for col in object_cols:\n        df[col] = label_encoder.fit_transform(df[col])\n    \n    # Ensure that categorical columns are set to 'category' dtype\n    category_cols = df.select_dtypes(include=['category']).columns\n    for col in category_cols:\n        df[col] = df[col].cat.codes\n    \n    return df\n\n# Apply the preparation to both the training and validation sets\n# X_train = prepare_data_for_xgb(X_train)\n# X_valid = prepare_data_for_xgb(X_valid)\nX = prepare_data_for_xgb(X)\ndf_test_processed_xgb = prepare_data_for_xgb(df_test_processed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the dataset into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import optuna\n# from xgboost import XGBClassifier\n# from sklearn.model_selection import train_test_split, cross_val_score\n# from sklearn.metrics import roc_auc_score\n\n# # Define objective function for Optuna\n# def objective(trial):\n#     # Define hyperparameters to search\n#     params = {\n#         'booster': 'gbtree',\n#         'objective': 'binary:logistic',\n#         'eval_metric': 'auc',\n#         'max_depth': trial.suggest_int('max_depth', 3, 20),\n#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n#         'min_child_weight': trial.suggest_float('min_child_weight', 1, 15),\n#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n#         'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 10.0),\n#         'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10.0),\n#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n#         'n_estimators': trial.suggest_int('n_estimators', 100, 12000),\n#         'device': 'cuda',\n#         'random_state': 0\n#     }\n\n#     # Split the training data into training and validation sets\n#     X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n#     # Train XGBoost model with current hyperparameters\n#     clf = XGBClassifier(**params)\n#     clf.fit(X_train_split, y_train_split)\n\n#     # Predict probabilities on validation set\n#     y_pred_proba = clf.predict_proba(X_valid_split)[:, 1]\n\n#     # Calculate ROC AUC on validation set\n#     roc_auc = roc_auc_score(y_valid_split, y_pred_proba)\n#     return roc_auc\n\n# # Optimize hyperparameters using Optuna\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=50)\n\n# # Get best hyperparameters\n# best_params = study.best_params\n# print(\"Best Hyperparameters:\", best_params)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Best Hyperparameters: {'max_depth': 3, 'learning_rate': 0.031499102014769985, 'min_child_weight': 8.038977881353192, 'colsample_bytree': 0.6503551622812244, 'reg_alpha': 3.5620122190330497, 'reg_lambda': 0.7367648328072658, 'subsample': 0.9730313232206979, 'n_estimators': 5495}","metadata":{}},{"cell_type":"code","source":"xgb_params_1 = {\n    'booster': 'gbtree',\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'alpha': 1.302348865795227e-06, \n    'max_depth': 15, \n    'learning_rate': 0.031499102014769985, \n    'subsample': 0.9730313232206979, \n    'colsample_bytree': 0.6503551622812244, \n    'min_child_weight': 8.03897788135319, \n    'gamma': 0.8399887056014855, \n    'reg_alpha': 3.5620122190330497, \n    'reg_lambda': 0.7367648328072658,\n    'n_estimators': 6000,\n    'max_bin': 71284,\n    'device': 'cuda'}\n# Split the training data to include a validation set for early stopping\nX_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n# Create the XGBoost model using Optuna model\nxgb_1 = XGBClassifier(**xgb_params_1)\n\n# Fit the model with early stopping\nxgb_1.fit(X_train_split, y_train_split,\n          eval_set=[(X_valid_split, y_valid_split)],\n          early_stopping_rounds=50,\n          verbose=500)\n\n# Predict probabilities on validation data\ny_pred_proba_xgb_1 = xgb_1.predict_proba(X_valid)[:, 1]\n\n# Calculate ROC AUC on validation data\nroc_auc = roc_auc_score(y_valid, y_pred_proba_xgb_1)\nprint(\"ROC AUC on Validation Data:\", roc_auc)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_params_2 = {    \n    'alpha': 1.302348865795227e-06, \n    'max_depth': 15, \n    'learning_rate': 0.061800451723613786, \n    'subsample': 0.7098803046786328, \n    'colsample_bytree': 0.2590672912533101, \n    'min_child_weight': 10, \n    'gamma': 0.8399887056014855, \n    'reg_alpha': 0.0016943548302122801, \n    'n_estimators': 12000,\n    'max_bin': 71284,\n    'device': 'cuda',\n    'eval_metric': 'auc'}\n\n# Split the training data to include a validation set for early stopping\nX_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n# Create the XGBoost model using Optuna model\nxgb_2 = XGBClassifier(**xgb_params_2)\n\n# Fit the model with early stopping\nxgb_2.fit(X_train_split, y_train_split,\n          eval_set=[(X_valid_split, y_valid_split)],\n          early_stopping_rounds=50,\n          verbose=500)\n\n# Predict probabilities on validation data\ny_pred_proba_xgb_2 = xgb_2.predict_proba(X_valid)[:, 1]\n\n# Calculate ROC AUC on validation data\nroc_auc = roc_auc_score(y_valid, y_pred_proba_xgb_2)\nprint(\"ROC AUC on Validation Data:\", roc_auc)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_params_3 = {    \n        'n_estimators': 6000,\n        'gamma': 0.1140906606618096, \n        'max_depth': 7, \n        'subsample': 0.9007710561094291, \n        'min_child_weight': 4, \n        'colsample_bytree': 0.492159477753482, \n        'learning_rate': 0.0034364805046499507, \n        'reg_lambda': 1.3534661601447577e-08, \n        'reg_alpha': 0.0028669447298327326,\n        'eval_metric': 'auc',\n        'objective': 'binary:logistic',\n        'tree_method': 'hist',\n        'verbosity': 0,\n        'random_state': 42\n    }\n\n# Split the training data to include a validation set for early stopping\nX_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n# Create the XGBoost model using Optuna model\nxgb_3 = XGBClassifier(**xgb_params_3)\n\n# Fit the model with early stopping\nxgb_3.fit(X_train_split, y_train_split,\n          eval_set=[(X_valid_split, y_valid_split)],\n          early_stopping_rounds=50,\n          verbose=500)\n\n# Predict probabilities on validation data\ny_pred_proba_xgb_3 = xgb_3.predict_proba(X_valid)[:, 1]\n\n# Calculate ROC AUC on validation data\nroc_auc = roc_auc_score(y_valid, y_pred_proba_xgb_3)\nprint(\"ROC AUC on Validation Data:\", roc_auc)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"lgb_params_1 = {\n        'n_estimators': 9000,\n        'max_depth': 14, \n        'min_samples_leaf': 67, \n        'subsample': 0.042393422213843186, \n        'learning_rate': 0.030646659464802262, \n        'lambda_l1': 3.5345645291665595, \n        'lambda_l2': 0.00020368766430271391, \n        'colsample_bytree': 0.7203603157340409,\n        'n_jobs': -1,\n        'objective': 'binary',\n        'metric': 'auc',\n        'boosting_type': 'gbdt',\n        'random_state': 42,\n        'verbose':-1\n    }\n\n# Split the training data to include a validation set for early stopping\nX_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n# Create the LGBM model using Optuna model\nlgb_1 = LGBMClassifier(**lgb_params_1)\n\n# Fit the model with early stopping\nlgb_1.fit(X_train_split, y_train_split,\n          eval_set=[(X_valid_split, y_valid_split)],\n          early_stopping_rounds=50)\n\n# Predict probabilities on validation data\ny_pred_proba_lgb_1 = lgb_1.predict_proba(X_valid)[:, 1]\n\n# Calculate ROC AUC on validation data\nroc_auc = roc_auc_score(y_valid, y_pred_proba_lgb_1)\nprint(\"ROC AUC on Validation Data:\", roc_auc)","metadata":{"scrolled":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb_params_2 = {\n        'n_estimators':  9000,\n        'max_depth': 12, \n        'min_samples_leaf': 98, \n        'subsample': 0.1518586091052909, \n        'learning_rate': 0.043176290991101526, \n        'lambda_l1': 4.617981401508956, \n        'lambda_l2': 0.00011687611486524523, \n        'colsample_bytree': 0.7166117501115165,\n        'objective': 'binary',\n        'metric': 'auc',\n        'boosting_type': 'gbdt',\n        'random_state': 42,\n        'verbose':-1\n    }\n\n# Split the training data to include a validation set for early stopping\nX_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n# Create the XGBoost model using Optuna model\nlgb_2 = LGBMClassifier(**lgb_params_2)\n\n# Fit the model with early stopping\nlgb_2.fit(X_train_split, y_train_split,\n          eval_set=[(X_valid_split, y_valid_split)],\n          early_stopping_rounds=50)\n\n# Predict probabilities on validation data\ny_pred_proba_lgb_2 = lgb_2.predict_proba(X_valid)[:, 1]\n\n# Calculate ROC AUC on validation data\nroc_auc = roc_auc_score(y_valid, y_pred_proba_lgb_2)\nprint(\"ROC AUC on Validation Data:\", roc_auc)","metadata":{"_kg_hide-output":true,"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb_params_3 = {\n         'n_estimators':  9000,\n         'learning_rate': 0.09600486056913297, \n         'num_leaves': 47, \n         'max_depth': 3, \n         'subsample_for_bin': 192138, \n         'reg_alpha': 0.06892148102293298, \n         'reg_lambda': 0.020608443619531975, \n         'feature_fraction': 0.3420095210806521, \n         'bagging_fraction': 0.8337974859410643, \n         'bagging_freq': 83, \n         'min_child_samples': 95\n}\n\n# Split the training data to include a validation set for early stopping\nX_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n# Create the XGBoost model using Optuna model\nlgb_3 = LGBMClassifier(**lgb_params_3)\n\n# Fit the model with early stopping\nlgb_3.fit(X_train_split, y_train_split,\n          eval_set=[(X_valid_split, y_valid_split)],\n          early_stopping_rounds=50)\n\n# Predict probabilities on validation data\ny_pred_proba_lgb_3 = lgb_3.predict_proba(X_valid)[:, 1]\n\n# Calculate ROC AUC on validation data\nroc_auc = roc_auc_score(y_valid, y_pred_proba_lgb_3)\nprint(\"ROC AUC on Validation Data:\", roc_auc)","metadata":{"_kg_hide-output":true,"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CatBoost","metadata":{}},{"cell_type":"code","source":"cat_params_1 = {\n        'iterations': 9999,\n        'depth': 5, \n        'min_data_in_leaf': 4, \n        'learning_rate': 0.03154964202059411, \n        'random_strength': 0.5184387965554558, \n        'rsm':1,\n        'l2_leaf_reg': 0.9077988480819391, \n        'grow_policy': 'Depthwise',\n        'bootstrap_type': 'Bayesian',\n        'od_type': 'Iter',\n        'eval_metric': 'AUC',\n        'loss_function': 'Logloss',\n        'random_state': 42,\n        'verbose':0\n}\n\n# Split the training data to include a validation set for early stopping\nX_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n# Create the XGBoost model using Optuna model\ncat_1 = CatBoostClassifier(**cat_params_1)\n\n# Fit the model with early stopping\ncat_1.fit(X_train_split, y_train_split,\n          eval_set=[(X_valid_split, y_valid_split)],\n          early_stopping_rounds=50)\n\n# Predict probabilities on validation data\ny_pred_proba_cat_1 = cat_1.predict_proba(X_valid)[:, 1]\n\n# Calculate ROC AUC on validation data\nroc_auc = roc_auc_score(y_valid, y_pred_proba_cat_1)\nprint(\"ROC AUC on Validation Data:\", roc_auc)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_params_2 = {\n        'iterations': 9999,\n        'loss_function'       : 'Logloss',\n        'eval_metric'         : \"AUC\",\n        'bagging_temperature' : 0.25,\n        'colsample_bylevel'   : 0.40,\n        'iterations'          : 5_000,\n        'learning_rate'       : 0.045,\n        'max_depth'           : 7,\n        'l2_leaf_reg'         : 0.80,\n        'min_data_in_leaf'    : 30,\n        'random_strength'     : 0.25,\n        'random_state'        : 42,\n        'early_stopping_rounds': 200,\n        'use_best_model'       : True\n}\n\n# Split the training data to include a validation set for early stopping\nX_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n# Create the XGBoost model using Optuna model\ncat_2 = CatBoostClassifier(**cat_params_1)\n\n# Fit the model with early stopping\ncat_2.fit(X_train_split, y_train_split,\n          eval_set=[(X_valid_split, y_valid_split)],\n          early_stopping_rounds=50)\n\n# Predict probabilities on validation data\ny_pred_proba_cat_2 = cat_2.predict_proba(X_valid)[:, 1]\n\n# Calculate ROC AUC on validation data\nroc_auc = roc_auc_score(y_valid, y_pred_proba_cat_2)\nprint(\"ROC AUC on Validation Data:\", roc_auc)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ensemble Models","metadata":{}},{"cell_type":"markdown","source":"## Soft Voting","metadata":{}},{"cell_type":"code","source":"y_preds = [y_pred_proba_xgb_1, y_pred_proba_xgb_2, y_pred_proba_xgb_3, y_pred_proba_lgb_1,y_pred_proba_lgb_2, y_pred_proba_lgb_3, y_pred_proba_cat_1, y_pred_proba_cat_2]\n\ny_pred_ensemble = (y_pred_proba_xgb_1 +\n                y_pred_proba_xgb_2 +\n                y_pred_proba_xgb_3 +\n                y_pred_proba_lgb_1 +\n                y_pred_proba_lgb_2 +\n                y_pred_proba_lgb_3 +\n                y_pred_proba_cat_1 +\n                y_pred_proba_cat_2) / len(y_preds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate ROC AUC on validation data\nroc_auc = roc_auc_score(y_valid, y_pred_ensemble)\nprint(\"ROC AUC on Validation Data:\", roc_auc)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Weighted Average","metadata":{}},{"cell_type":"code","source":"weights = [0.15, 0.05, 0.15, 0.15, 0.05, 0.05, 0.1, 0.1, 0.2]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_ensemble = (y_pred_proba_xgb_1 * weights[0] +\n                y_pred_proba_xgb_2 * weights[1] +\n                y_pred_proba_xgb_3 * weights[2] +\n                y_pred_proba_lgb_1 * weights[3] +\n                y_pred_proba_lgb_2 * weights[4] +\n                y_pred_proba_lgb_3 * weights[5] +\n                y_pred_proba_cat_1 * weights[6] +\n                y_pred_proba_cat_2 * weights[7] )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate ROC AUC on validation data\nroc_auc = roc_auc_score(y_valid, y_pred_ensemble)\nprint(\"ROC AUC on Validation Data:\", roc_auc)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"y_pred_test_xgb_1 = xgb_1.predict_proba(df_test_processed_xgb)[:, 1]\ny_pred_test_xgb_2 = xgb_2.predict_proba(df_test_processed_xgb)[:, 1]\ny_pred_test_xgb_3 = xgb_3.predict_proba(df_test_processed_xgb)[:, 1]\ny_pred_test_lgb_1 = lgb_1.predict_proba(df_test_processed_xgb)[:, 1]\ny_pred_test_lgb_2 = lgb_2.predict_proba(df_test_processed_xgb)[:, 1]\ny_pred_test_lgb_3 = lgb_3.predict_proba(df_test_processed_xgb)[:, 1]\ny_pred_test_cat_1 = cat_1.predict_proba(df_test_processed_xgb)[:, 1]\ny_pred_test_cat_2 = cat_2.predict_proba(df_test_processed_xgb)[:, 1]\ny_pred_test_aml = automl.predict(df_test_processed).data[:, 0]\n\nweights = [0.5, 0, 0, 0, 0, 0, 0, 0, 0.5]\n\n# Combine predictions using weighted average\ny_pred_test_ensemble = (y_pred_test_xgb_1 * weights[0] +\n                y_pred_test_xgb_2 * weights[1] +\n                y_pred_test_xgb_3 * weights[2] +\n                y_pred_test_lgb_1 * weights[3] +\n                y_pred_test_lgb_2 * weights[4] +\n                y_pred_test_lgb_3 * weights[5] +\n                y_pred_test_cat_1 * weights[6] +\n                y_pred_test_cat_2 * weights[7] +\n                y_pred_test_aml * weights[8])\n\n# Create submission file\ndf_sub = pd.read_csv('/kaggle/input/playground-series-s4e10/sample_submission.csv')\ndf_sub['loan_status'] = y_pred_test_ensemble\ndf_sub.to_csv('submission_ensemble.csv', index=False)\ndf_sub.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"execution_count":null}]}