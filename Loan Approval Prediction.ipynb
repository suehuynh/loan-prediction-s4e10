{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84894,"databundleVersionId":9709193,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/suehuynh/kaggle-s4e10-loan-approval-prediction?scriptVersionId=214336659\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd              # For data manipulation and analysis\nimport numpy as np               # For numerical computing\nfrom datetime import datetime\nimport scipy.stats as stats      # For statistical analysis\nimport math\nimport matplotlib                # For plotting and visualization\nimport matplotlib.pyplot as plt  \nfrom pandas.plotting import parallel_coordinates\nimport seaborn as sns            # For statistical data visualization\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-12-22T18:33:54.309853Z","iopub.execute_input":"2024-12-22T18:33:54.31025Z","iopub.status.idle":"2024-12-22T18:33:57.291918Z","shell.execute_reply.started":"2024-12-22T18:33:54.310203Z","shell.execute_reply":"2024-12-22T18:33:57.290848Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For machine learning\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier, Pool\n\nfrom lightgbm import early_stopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, roc_auc_score,\n                             f1_score, confusion_matrix, classification_report)\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.compose import ColumnTransformer\nimport optuna","metadata":{"execution":{"iopub.status.busy":"2024-12-22T18:33:57.293636Z","iopub.execute_input":"2024-12-22T18:33:57.294066Z","iopub.status.idle":"2024-12-22T18:33:59.164397Z","shell.execute_reply.started":"2024-12-22T18:33:57.294034Z","shell.execute_reply":"2024-12-22T18:33:59.163555Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/playground-series-s4e10/train.csv',index_col=0)\ndf_test = pd.read_csv('/kaggle/input/playground-series-s4e10/test.csv', index_col=0)","metadata":{"execution":{"iopub.status.busy":"2024-12-22T18:33:59.165591Z","iopub.execute_input":"2024-12-22T18:33:59.166089Z","iopub.status.idle":"2024-12-22T18:33:59.404949Z","shell.execute_reply.started":"2024-12-22T18:33:59.166056Z","shell.execute_reply":"2024-12-22T18:33:59.404105Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.concat([df_train, df_test])","metadata":{"execution":{"iopub.status.busy":"2024-12-22T18:34:28.76157Z","iopub.execute_input":"2024-12-22T18:34:28.762603Z","iopub.status.idle":"2024-12-22T18:34:28.773924Z","shell.execute_reply.started":"2024-12-22T18:34:28.76255Z","shell.execute_reply":"2024-12-22T18:34:28.772887Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-12-22T18:34:30.600352Z","iopub.execute_input":"2024-12-22T18:34:30.600744Z","iopub.status.idle":"2024-12-22T18:34:30.641784Z","shell.execute_reply.started":"2024-12-22T18:34:30.600708Z","shell.execute_reply":"2024-12-22T18:34:30.640713Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T19:42:38.636433Z","iopub.execute_input":"2024-10-22T19:42:38.637051Z","iopub.status.idle":"2024-10-22T19:42:38.672863Z","shell.execute_reply.started":"2024-10-22T19:42:38.637007Z","shell.execute_reply":"2024-10-22T19:42:38.667368Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['loan_status'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T19:42:38.677643Z","iopub.execute_input":"2024-10-22T19:42:38.678574Z","iopub.status.idle":"2024-10-22T19:42:38.696243Z","shell.execute_reply.started":"2024-10-22T19:42:38.678507Z","shell.execute_reply":"2024-10-22T19:42:38.694701Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Univariate Analysis\nPerform distribution analysis on numerical features and Target - Loan Status","metadata":{}},{"cell_type":"code","source":"num_cols = [col for col in df.columns if df[col].dtypes in ['int', 'float']]\ncat_cols = [col for col in df.columns if col not in num_cols]\nprint(num_cols, cat_cols)","metadata":{"execution":{"iopub.status.busy":"2024-12-22T18:34:33.975074Z","iopub.execute_input":"2024-12-22T18:34:33.976033Z","iopub.status.idle":"2024-12-22T18:34:33.98201Z","shell.execute_reply.started":"2024-12-22T18:34:33.975993Z","shell.execute_reply":"2024-12-22T18:34:33.980874Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(20, 15))\n\nfor i, col in enumerate(num_cols):\n    ax = sns.histplot(data=df,\n                      x=col,\n                      bins=20,\n                      ax=axes[i // 2, i % 2])\n    ax.bar_label(ax.containers[1])\n\nfig.tight_layout(h_pad=2)\nplt.subplots_adjust(top=0.92)\nplt.suptitle('Numerical Feature Distributions', fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T19:42:38.710341Z","iopub.execute_input":"2024-10-22T19:42:38.710827Z","iopub.status.idle":"2024-10-22T19:42:42.448519Z","shell.execute_reply.started":"2024-10-22T19:42:38.710774Z","shell.execute_reply":"2024-10-22T19:42:42.447239Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Observations\n- `person_age` column and `person_emp_length` column have an extreme outlier to be removed","metadata":{}},{"cell_type":"markdown","source":"Analyze proportion of each categorical feature","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 15))\n\naxes = axes.flatten()\n\nfor i, col in enumerate(cat_cols):\n    # Get the counts for each category in the column\n    counts = df[col].value_counts()\n    # Plot the pie chart\n    axes[i].pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=90, counterclock=False)\n    # Set title to the column name\n    axes[i].set_title(f'{col} Distribution', fontsize=14)\n    \n\nfig.tight_layout(h_pad=2)\nplt.subplots_adjust(top=0.92)\nplt.suptitle('Categorical Feature Distributions', fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T19:42:42.449911Z","iopub.execute_input":"2024-10-22T19:42:42.450297Z","iopub.status.idle":"2024-10-22T19:42:43.542561Z","shell.execute_reply.started":"2024-10-22T19:42:42.450256Z","shell.execute_reply":"2024-10-22T19:42:43.541026Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Observations\n- `loan_intent` column has an quite even distribution between each category. We can use One Hot Coding for this feature.\n- The majority of loan applicants `person_home_ownership` is Rent or Mortgage. We will look into the correlation this feature and the loan status to decide the engineering tactic.\n- `cb_person_default_on_file` can be binary coded\n- `loan_grade` column can be simplified by grouping (A,B), (C,D), and (E,F,G)\n- `person_home_ownership`","metadata":{}},{"cell_type":"markdown","source":"## Multivariate Analysis","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 4, figsize=(20, 15))\n \nfor i, col in enumerate(num_cols):\n    plt.subplots_adjust(top = 0.85)\n    ax = sns.histplot(data = df, \n                x = col, \n                hue = 'loan_status',\n                bins = 20,\n                ax = axes[i // 4, i % 4])\n    ax.set_yticklabels(['{:,.0f}K'.format(ticks / 1000) for ticks in ax.get_yticks()])\nfig.tight_layout(h_pad=2)\nplt.subplots_adjust(top=0.92)\nplt.suptitle('Numerical Feature Distributions by Target', fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T19:42:43.544592Z","iopub.execute_input":"2024-10-22T19:42:43.545078Z","iopub.status.idle":"2024-10-22T19:42:47.105317Z","shell.execute_reply.started":"2024-10-22T19:42:43.545033Z","shell.execute_reply":"2024-10-22T19:42:47.104017Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Observations\n- Distribution of numerical features between Approved (1) and Disapproved (0) is similar for `person_age`, `person_income`, `person_emp_length`, `cb_person_cred_hist_length`, `loan_amnt`\n- `loan_int_rate` and `loan_percent_income` is more left-tailed for Approved applicants than for Disapproved ones.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n \nfor i, col in enumerate(cat_cols):\n    plt.subplots_adjust(top = 0.85)\n    ax = sns.histplot(data = df, \n                x = col, \n                hue = 'loan_status',\n                ax = axes[i // 2, i % 2])\n\nfig.tight_layout(h_pad=2)\nplt.subplots_adjust(top=0.92)\nplt.suptitle('Categorical Feature Distributions by Target', fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T19:42:47.106905Z","iopub.execute_input":"2024-10-22T19:42:47.107302Z","iopub.status.idle":"2024-10-22T19:42:49.320468Z","shell.execute_reply.started":"2024-10-22T19:42:47.107261Z","shell.execute_reply":"2024-10-22T19:42:49.319159Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"num_cols.remove('loan_status')","metadata":{"execution":{"iopub.status.busy":"2024-12-22T18:34:39.526734Z","iopub.execute_input":"2024-12-22T18:34:39.527093Z","iopub.status.idle":"2024-12-22T18:34:39.531731Z","shell.execute_reply.started":"2024-12-22T18:34:39.527062Z","shell.execute_reply":"2024-12-22T18:34:39.530684Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def feature_engineer(df):\n    # 1. Age Group\n    df['age_group'] = pd.cut(df['person_age'], bins=[0, 24, 34, 44, 54, 100], \n                             labels=['<25', '25-34', '35-44', '45-54', '55+'])\n\n    # 2. Income Level\n    df['income_level'] = pd.cut(df['person_income'], bins=[0, 30000, 60000, 100000, float('inf')], \n                                labels=['Low', 'Medium', 'High', 'Very High'])\n\n    # 3. Employment Stability\n    df['employment_stability'] = pd.cut(df['person_emp_length'], bins=[0, 1, 5, 10, float('inf')],\n                                        labels=['<1 year', '1-5 years', '6-10 years', '10+ years'])\n\n    # 4. Loan Amount Range\n    df['loan_amnt_range'] = pd.cut(df['loan_amnt'], bins=[0, 5000, 10000, 20000, float('inf')],\n                                   labels=['<5k', '5-10k', '10-20k', '20k+'])\n\n    # 5. Interest Rate Buckets\n    df['loan_int_rate_range'] = pd.cut(df['loan_int_rate'], bins=[0, 5, 10, 15, 20, float('inf')],\n                                       labels=['<5%', '5-10%', '10-15%', '15-20%', '20%+'])\n\n    # 6. Loan-to-Income Ratio Buckets\n    df['loan_percent_income_range'] = pd.cut(df['loan_percent_income'], bins=[0, 0.2, 0.4, 0.6, float('inf')],\n                                             labels=['<20%', '20-40%', '40-60%', '60%+'])\n\n    # 7. Encode Default History\n    df['default_history'] = df['cb_person_default_on_file'].map({'N': 0, 'Y': 1})\n\n    # 8. Credit History Length Category\n    df['credit_hist_length_cat'] = pd.cut(df['cb_person_cred_hist_length'], bins=[0, 5, 10, 15, float('inf')],\n                                          labels=['<5 years', '5-10 years', '10-15 years', '15+ years'])\n\n    # 9. Interaction Feature: Income Level & Employment Stability\n    df['income_emp_interaction'] = df['income_level'].astype(str) + '_' + df['employment_stability'].astype(str)\n\n    # 10. Interaction Feature: Loan Grade & Interest Rate\n    df['grade_int_rate'] = df['loan_grade'] + '_' + df['loan_int_rate_range'].astype(str)\n\n    # 11. Debt Burden (Loan Amount Relative to Income)\n    df['debt_burden'] = df['loan_amnt'] / df['person_income']\n\n    # 12. Credit History per Age\n    df['credit_hist_per_age'] = df['cb_person_cred_hist_length'] / df['person_age']\n\n    # 13. Risk Score (Combining multiple risk factors)\n    df['risk_score'] = (\n        df['loan_int_rate'] * df['loan_percent_income'] / (df['cb_person_cred_hist_length'] + 1)\n    )\n\n    # 14. Aggregated Default Risk\n    df['high_risk_flag'] = ((df['default_history'] == 1) | \n                            (df['loan_int_rate'] > 15) |\n                            (df['loan_percent_income'] > 0.5)).astype(int)\n\n    # 15. Age-Adjusted Employment Length\n    df['emp_length_age_ratio'] = df['person_emp_length'] / df['person_age']\n\n    # 17. Loan Amount per Year of Employment\n    df['loan_per_year_emp'] = df['loan_amnt'] / (df['person_emp_length'] + 1)\n\n    # 18. Age-to-Income Ratio\n    df['age_to_income_ratio'] = df['person_age'] / df['person_income']\n\n    # 19. Adjusted Interest Rate (Interest Rate adjusted by Loan Grade)\n    loan_grade_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n    df['loan_grade_numeric'] = df['loan_grade'].map(loan_grade_map)\n    df['adjusted_interest_rate'] = df['loan_int_rate'] / (df['loan_grade_numeric'] + 1)\n\n    # 20. Credit Age Relative to Employment Length\n    df['credit_to_emp_ratio'] = df['cb_person_cred_hist_length'] / (df['person_emp_length'] + 1)\n\n    # 21. Financial Stability Index (FSI)\n    # A composite score combining income, loan amount, and interest rate to assess financial stability\n    df['financial_stability_index'] = df['person_income'] / (df['loan_amnt'] * df['loan_int_rate'])\n\n    # 22. Loan Amortization Speed (Hypothetical)\n    # The ratio of income to loan amount, indicating how quickly the loan might be paid off\n    df['loan_amortization_speed'] = df['person_income'] / df['loan_amnt']\n\n    # 23. Loan Purpose Risk Weight (Assign risk score based on loan intent)\n    loan_intent_risk_map = {'EDUCATION': 0.5, 'MEDICAL': 0.8, 'PERSONAL': 1.0, 'HOMEIMPROVEMENT': 0.6, \n                            'VENTURE': 1.5, 'DEBTCONSOLIDATION': 0.7}\n    df['loan_intent_risk'] = df['loan_intent'].map(loan_intent_risk_map)\n\n    # 24. Employment to Income Ratio\n    df['emp_income_ratio'] = df['person_emp_length'] / (df['person_income'] / 1000)\n\n    return df\n\ndf_train_processed = feature_engineer(df_train)\ndf_test_processed = feature_engineer(df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T18:34:41.757591Z","iopub.execute_input":"2024-12-22T18:34:41.758001Z","iopub.status.idle":"2024-12-22T18:34:41.944742Z","shell.execute_reply.started":"2024-12-22T18:34:41.757966Z","shell.execute_reply":"2024-12-22T18:34:41.943679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df_train_processed.drop(columns = ['loan_status'] , axis = 1)\ny = df_train_processed['loan_status']\n\n# Split the dataset into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T18:34:46.415081Z","iopub.execute_input":"2024-12-22T18:34:46.415514Z","iopub.status.idle":"2024-12-22T18:34:46.485985Z","shell.execute_reply.started":"2024-12-22T18:34:46.415455Z","shell.execute_reply":"2024-12-22T18:34:46.484815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Initialize the StandardScaler\nscaler = StandardScaler()\n\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\nX_valid[num_cols] = scaler.transform(X_valid[num_cols])\ndf_test_processed[num_cols] = scaler.transform(df_test_processed[num_cols])\n\nfrom sklearn.preprocessing import LabelEncoder\n\ndef prepare_data_for_xgb(df):\n    # Convert object columns to categorical if needed\n    object_cols = df.select_dtypes(include='object').columns\n    \n    # Use LabelEncoder or pd.get_dummies to encode categorical columns\n    label_encoder = LabelEncoder()\n    \n    for col in object_cols:\n        df[col] = label_encoder.fit_transform(df[col])\n    \n    # Ensure that categorical columns are set to 'category' dtype\n    category_cols = df.select_dtypes(include=['category']).columns\n    for col in category_cols:\n        df[col] = df[col].cat.codes\n    \n    return df\n\n# Apply the preparation to both the training and validation sets\nX_train = prepare_data_for_xgb(X_train)\nX_valid = prepare_data_for_xgb(X_valid)\ndf_test_processed_xgb = prepare_data_for_xgb(df_test_processed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T18:34:48.363062Z","iopub.execute_input":"2024-12-22T18:34:48.364016Z","iopub.status.idle":"2024-12-22T18:34:48.576962Z","shell.execute_reply.started":"2024-12-22T18:34:48.363978Z","shell.execute_reply":"2024-12-22T18:34:48.576004Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import library\nfrom imblearn.over_sampling import SMOTE\nimport collections\nfrom collections import Counter\n\nsmote = SMOTE()\n\n# fit predictor and target variable\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\nprint('Original dataset shape', Counter(y_train))\nprint('Resample dataset shape', Counter(y_train_smote))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T18:35:08.537512Z","iopub.execute_input":"2024-12-22T18:35:08.538427Z","iopub.status.idle":"2024-12-22T18:35:09.23917Z","shell.execute_reply.started":"2024-12-22T18:35:08.538368Z","shell.execute_reply":"2024-12-22T18:35:09.237923Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Machine Learning","metadata":{}},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import roc_auc_score\n\n# Define objective function for Optuna\ndef objective(trial):\n    # Define hyperparameters to search\n    params = {\n        'booster': 'gbtree',\n        'objective': 'binary:logistic',\n        'eval_metric': 'auc',\n        'max_depth': trial.suggest_int('max_depth', 3, 20),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n        'min_child_weight': trial.suggest_float('min_child_weight', 1, 15),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10.0),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 12000),\n        'device': 'cuda',\n        'random_state': 0\n    }\n\n    # Split the training data into training and validation sets\n    X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n    # Train XGBoost model with current hyperparameters\n    clf = XGBClassifier(**params)\n    clf.fit(X_train_split, y_train_split)\n\n    # Predict probabilities on validation set\n    y_pred_proba = clf.predict_proba(X_valid_split)[:, 1]\n\n    # Calculate ROC AUC on validation set\n    roc_auc = roc_auc_score(y_valid_split, y_pred_proba)\n    return roc_auc\n\n# Optimize hyperparameters using Optuna\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\n\n# Get best hyperparameters\nbest_params = study.best_params\nprint(\"Best Hyperparameters:\", best_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T18:36:20.79274Z","iopub.execute_input":"2024-12-22T18:36:20.793403Z","iopub.status.idle":"2024-12-22T18:39:38.180182Z","shell.execute_reply.started":"2024-12-22T18:36:20.793364Z","shell.execute_reply":"2024-12-22T18:39:38.179055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the training data to include a validation set for early stopping\nX_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n# Create the XGBoost model using Optuna model\nxgb_1 = XGBClassifier(**best_params)\n\n# Fit the model with early stopping\nxgb_1.fit(X_train_split, y_train_split,\n          eval_set=[(X_valid_split, y_valid_split)],\n          early_stopping_rounds=50,\n          verbose=500)\n\n# Predict probabilities on validation data\ny_pred_proba_xgb_1 = xgb_1.predict_proba(X_valid)[:, 1]\n\n# Calculate ROC AUC on validation data\nroc_auc = roc_auc_score(y_valid, y_pred_proba_xgb_1)\nprint(\"ROC AUC on Validation Data:\", roc_auc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T18:42:04.651017Z","iopub.execute_input":"2024-12-22T18:42:04.652014Z","iopub.status.idle":"2024-12-22T18:42:11.679251Z","shell.execute_reply.started":"2024-12-22T18:42:04.651972Z","shell.execute_reply":"2024-12-22T18:42:11.678299Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LGBM","metadata":{}},{"cell_type":"code","source":"# Define objective function for Optuna\ndef objective(trial):\n    # Define hyperparameters to search\n    params = {\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'random_state': 0,\n        'verbose': -1\n    }\n\n    # Split the training data into training and validation sets\n    X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n    # Train LightGBM model with current hyperparameters\n    clf = LGBMClassifier(**params)\n    clf.fit(X_train_split, y_train_split)\n\n    # Predict probabilities on validation set\n    y_pred_proba = clf.predict_proba(X_valid_split)[:, 1]\n\n    # Calculate ROC AUC on validation set\n    roc_auc = roc_auc_score(y_valid_split, y_pred_proba)\n    return roc_auc\n\n# Optimize hyperparameters using Optuna\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\n\n# Get best hyperparameters\nbest_params = study.best_params\nprint(\"Best Hyperparameters:\", best_params)","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-22T18:42:27.162779Z","iopub.execute_input":"2024-12-22T18:42:27.163173Z","iopub.status.idle":"2024-12-22T18:42:55.110779Z","shell.execute_reply.started":"2024-12-22T18:42:27.163137Z","shell.execute_reply":"2024-12-22T18:42:55.109651Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightgbm import log_evaluation\n\n# Split the training data to include a validation set for early stopping\nX_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n# Create the XGBoost model using Optuna model\nlgb_1 = LGBMClassifier(**best_params)\n\n# Fit the model with early stopping\nlgb_1.fit(\n    X_train_split, y_train_split,\n    eval_set=[(X_valid_split, y_valid_split)],\n    callbacks=[early_stopping(100), log_evaluation(100)]\n)\n\n# Predict probabilities on validation data\ny_pred_proba_lgb_1 = lgb_1.predict_proba(X_valid)[:, 1]\n\n# Calculate ROC AUC on validation data\nroc_auc = roc_auc_score(y_valid, y_pred_proba_lgb_1)\nprint(\"ROC AUC on Validation Data:\", roc_auc) #0.9597419019693103","metadata":{"execution":{"iopub.status.busy":"2024-12-22T18:42:55.11286Z","iopub.execute_input":"2024-12-22T18:42:55.113589Z","iopub.status.idle":"2024-12-22T18:42:57.334603Z","shell.execute_reply.started":"2024-12-22T18:42:55.113537Z","shell.execute_reply":"2024-12-22T18:42:57.33352Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CatBoost","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import roc_auc_score\n\n# Define objective function for Optuna\ndef objective(trial):\n    # Define hyperparameters to search\n    params = {\n        'iterations': trial.suggest_int('iterations', 100, 1000),\n        'depth': trial.suggest_int('depth', 4, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'random_strength': trial.suggest_float('random_strength', 0.1, 10.0),\n        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n        'border_count': trial.suggest_int('border_count', 1, 255),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-5, 100),\n        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.01, 1.0),\n        'eval_metric': 'AUC',\n        'random_state': 0\n    }\n\n    # Split the training data into training and validation sets\n    X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n    # Train CatBoost model with current hyperparameters\n    clf = CatBoostClassifier(**params)\n    clf.fit(X_train_split, y_train_split, eval_set=(X_valid_split, y_valid_split), verbose=0, early_stopping_rounds=50)\n\n    # Predict probabilities on validation set\n    y_pred_proba = clf.predict_proba(X_valid_split)[:, 1]\n\n    # Calculate ROC AUC on validation set\n    roc_auc = roc_auc_score(y_valid_split, y_pred_proba)\n    return roc_auc\n\n# Optimize hyperparameters using Optuna\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\n\n# Get best hyperparameters\nbest_params = study.best_params\nprint(\"Best Hyperparameters:\", best_params)","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-22T18:42:57.338116Z","iopub.execute_input":"2024-12-22T18:42:57.338432Z","iopub.status.idle":"2024-12-22T18:43:16.470794Z","shell.execute_reply.started":"2024-12-22T18:42:57.338401Z","shell.execute_reply":"2024-12-22T18:43:16.469796Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the training data to include a validation set for early stopping\nX_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n\n# # Create the CatBoost model using Optuna model\ncat_1 = CatBoostClassifier(**best_params)\n\n# Fit the model with early stopping\ncat_1.fit(X_train_split, y_train_split,\n          eval_set=[(X_valid_split, y_valid_split)],\n          early_stopping_rounds=50,\n          verbose=500)\n\n# Predict probabilities on validation data\ny_pred_proba_cat_1 = cat_1.predict_proba(X_valid)[:, 1]\n\n# Calculate ROC AUC on validation data\nroc_auc = roc_auc_score(y_valid, y_pred_proba_cat_1)\nprint(\"ROC AUC on Validation Data:\", roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-12-22T18:44:28.928803Z","iopub.execute_input":"2024-12-22T18:44:28.929177Z","iopub.status.idle":"2024-12-22T18:44:33.357066Z","shell.execute_reply.started":"2024-12-22T18:44:28.929142Z","shell.execute_reply":"2024-12-22T18:44:33.356207Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Light AutoML","metadata":{}},{"cell_type":"code","source":"!pip install -U lightautoml[all]","metadata":{"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-12-22T18:44:50.221006Z","iopub.execute_input":"2024-12-22T18:44:50.221397Z","iopub.status.idle":"2024-12-22T18:47:57.896406Z","shell.execute_reply.started":"2024-12-22T18:44:50.221361Z","shell.execute_reply":"2024-12-22T18:47:57.894974Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightautoml.automl.presets.tabular_presets import TabularAutoML\nfrom lightautoml.tasks import Task\nimport torch\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-12-22T18:47:57.899073Z","iopub.execute_input":"2024-12-22T18:47:57.899607Z","iopub.status.idle":"2024-12-22T18:48:17.639305Z","shell.execute_reply.started":"2024-12-22T18:47:57.899547Z","shell.execute_reply":"2024-12-22T18:48:17.638466Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" \n    Iterate through all the columns of a dataframe and modify the data type\n    to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if pd.api.types.is_numeric_dtype(df[col]):\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if pd.api.types.is_integer_dtype(df[col]):\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                else:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('object')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-12-22T18:48:17.640571Z","iopub.execute_input":"2024-12-22T18:48:17.64121Z","iopub.status.idle":"2024-12-22T18:48:17.651876Z","shell.execute_reply.started":"2024-12-22T18:48:17.641173Z","shell.execute_reply":"2024-12-22T18:48:17.650793Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_processed[num_cols] = scaler.fit_transform(df_train_processed[num_cols])\ndf_train_processed = reduce_mem_usage(df_train_processed)\ndf_train_processed.info()","metadata":{"execution":{"iopub.status.busy":"2024-12-22T18:48:17.654473Z","iopub.execute_input":"2024-12-22T18:48:17.65533Z","iopub.status.idle":"2024-12-22T18:48:19.827708Z","shell.execute_reply.started":"2024-12-22T18:48:17.655278Z","shell.execute_reply":"2024-12-22T18:48:19.826709Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test_processed = reduce_mem_usage(df_test_processed)\ndf_test_processed.info()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T00:57:05.90453Z","iopub.execute_input":"2024-10-22T00:57:05.905957Z","iopub.status.idle":"2024-10-22T00:57:05.966187Z","shell.execute_reply.started":"2024-10-22T00:57:05.905894Z","shell.execute_reply":"2024-10-22T00:57:05.964442Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def map_class(x, task, reader):\n    if task.name == 'multiclass':\n        return reader[x]\n    else:\n        return x\n\nmapped = np.vectorize(map_class)\n\ndef score(task, y_true, y_pred):\n    if task.name == 'binary':\n        return roc_auc_score(y_true, y_pred)\n    elif task.name == 'multiclass':\n        return accuracy_score(y_true, np.argmax(y_pred, 1))\n    elif task.name == 'reg' or task.name == 'multi:reg':\n        return median_absolute_error(y_true, y_pred)\n    else:\n        raise 'Task is not correct.'\n        \ndef take_pred_from_task(pred, task):\n    if task.name == 'binary' or task.name == 'reg':\n        return pred[:, 0]\n    elif task.name == 'multiclass' or task.name == 'multi:reg':\n        return pred\n    else:\n        raise 'Task is not correct.'\n        \ndef use_plr(USE_PLR):\n    if USE_PLR:\n        return \"plr\"\n    else:\n        return \"cont\"","metadata":{"execution":{"iopub.status.busy":"2024-10-22T00:57:09.980499Z","iopub.execute_input":"2024-10-22T00:57:09.9815Z","iopub.status.idle":"2024-10-22T00:57:09.992558Z","shell.execute_reply.started":"2024-10-22T00:57:09.981445Z","shell.execute_reply":"2024-10-22T00:57:09.991295Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RANDOM_STATE = 42\nN_THREADS = os.cpu_count()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T00:57:10.89855Z","iopub.execute_input":"2024-10-22T00:57:10.90016Z","iopub.status.idle":"2024-10-22T00:57:10.910588Z","shell.execute_reply.started":"2024-10-22T00:57:10.90007Z","shell.execute_reply":"2024-10-22T00:57:10.909132Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T00:57:11.870912Z","iopub.execute_input":"2024-10-22T00:57:11.872618Z","iopub.status.idle":"2024-10-22T00:57:11.897682Z","shell.execute_reply.started":"2024-10-22T00:57:11.872531Z","shell.execute_reply":"2024-10-22T00:57:11.896395Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"task = Task('binary') \nautoml = TabularAutoML(\n    task = task, \n    timeout = 9 * 3600,\n    cpu_limit = os.cpu_count(),\n    nn_params = {\n        \"n_epochs\": 10, \n        \"bs\": 1024, \n        \"num_workers\": 0, \n        \"path_to_save\": None, \n        \"freeze_defaults\": True,\n        \"cont_embedder\": 'plr',\n        'cat_embedder': 'weighted',\n        \"hidden_size\": 64,\n        'hid_factor': [3, 3],\n        'block_config': [3, 3],\n        'embedding_size': 64, \n        'stop_by_metric': True,\n        'verbose_bar': True,\n        \"snap_params\": { 'k': 2, 'early_stopping': True, 'patience': 2, 'swa': True }\n    },\n    nn_pipeline_params = {\"use_qnt\": False, \"use_te\": False},\n    reader_params = {'n_jobs': os.cpu_count(), 'cv': 10, 'random_state': 42, 'advanced_roles': True}\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T00:57:12.646853Z","iopub.execute_input":"2024-10-22T00:57:12.64765Z","iopub.status.idle":"2024-10-22T00:57:12.73369Z","shell.execute_reply.started":"2024-10-22T00:57:12.647572Z","shell.execute_reply":"2024-10-22T00:57:12.732371Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"out_of_fold_predictions = automl.fit_predict(\n    df_train_processed,\n    roles = {\n        'target': 'loan_status',\n    }, \n    verbose = 3\n)","metadata":{"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-10-22T01:30:21.505007Z","iopub.execute_input":"2024-10-22T01:30:21.505555Z","iopub.status.idle":"2024-10-22T01:45:20.877525Z","shell.execute_reply.started":"2024-10-22T01:30:21.505506Z","shell.execute_reply":"2024-10-22T01:45:20.876075Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Light AutoML Accuracy:\")\nroc_auc_score(df_train_processed.loan_status, out_of_fold_predictions.data)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:46:19.398074Z","iopub.execute_input":"2024-10-22T01:46:19.398561Z","iopub.status.idle":"2024-10-22T01:46:19.431866Z","shell.execute_reply.started":"2024-10-22T01:46:19.398515Z","shell.execute_reply":"2024-10-22T01:46:19.43052Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"markdown","source":"## Soft Voting","metadata":{}},{"cell_type":"code","source":"# Combine predictions using soft voting\ny_pred_softvoting = (y_pred_proba_xgb_1 + y_pred_proba_lgb_1 + y_pred_proba_cat_1) / 3\n\n# Calculate ROC AUC on validation data\nroc_auc_softvoting = roc_auc_score(y_valid, y_pred_softvoting)\nprint(\"ROC AUC on Validation Data (Soft Voting):\", roc_auc_softvoting)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T00:35:41.453762Z","iopub.status.idle":"2024-10-22T00:35:41.454325Z","shell.execute_reply.started":"2024-10-22T00:35:41.454087Z","shell.execute_reply":"2024-10-22T00:35:41.454113Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Weighted Average","metadata":{}},{"cell_type":"code","source":"# Define weights for the models\nweights = [0.4, 0.5, 0.15] \n\n# Combine predictions using weighted average\ny_pred_weighted = (weights[0] * y_pred_proba_xgb_1 +\n                   weights[1] * y_pred_proba_lgb_1 +\n                   weights[2] * y_pred_proba_cat_1)\n\n# Calculate ROC AUC on validation data\nroc_auc_weighted = roc_auc_score(y_valid, y_pred_weighted)\nprint(\"ROC AUC on Validation Data (Weighted Average):\", roc_auc_weighted)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T00:35:41.456344Z","iopub.status.idle":"2024-10-22T00:35:41.456814Z","shell.execute_reply.started":"2024-10-22T00:35:41.456596Z","shell.execute_reply":"2024-10-22T00:35:41.456617Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"roc_auc_ensemble = {\n    'Soft Voting': roc_auc_softvoting,\n    'Weighted Average': roc_auc_weighted\n}\n\nprint(\"Ensemble Accuracy:\")\nfor method, roc in roc_auc_ensemble.items():\n    print(f\"{method}: {roc:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-22T00:35:41.458866Z","iopub.status.idle":"2024-10-22T00:35:41.459508Z","shell.execute_reply.started":"2024-10-22T00:35:41.459204Z","shell.execute_reply":"2024-10-22T00:35:41.459237Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n# Calculate ROC curve\nfpr, tpr, thresholds = roc_curve(y_valid, y_pred_softvoting) \nroc_auc = auc(fpr, tpr)\n# Plot the ROC curve\nplt.figure()  \nplt.plot(fpr, tpr, label='ROC curve (area = %0.4f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--', label='No Skill')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Loan Type Classification - Soft Voting')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T00:35:41.463116Z","iopub.status.idle":"2024-10-22T00:35:41.463539Z","shell.execute_reply.started":"2024-10-22T00:35:41.463346Z","shell.execute_reply":"2024-10-22T00:35:41.463367Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n# Calculate ROC curve\nfpr, tpr, thresholds = roc_curve(y_valid, y_pred_weighted) \nroc_auc = auc(fpr, tpr)\n# Plot the ROC curve\nplt.figure()  \nplt.plot(fpr, tpr, label='ROC curve (area = %0.4f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--', label='No Skill')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Loan Type Classification - Weighted')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T00:35:41.464758Z","iopub.status.idle":"2024-10-22T00:35:41.465201Z","shell.execute_reply.started":"2024-10-22T00:35:41.465001Z","shell.execute_reply":"2024-10-22T00:35:41.465024Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Predict probabilities on test data for both models\ny_pred_test_xgb = xgb_1.predict_proba(df_test_processed)[:, 1]\ny_pred_test_lgb = lgb_1.predict_proba(df_test_processed)[:, 1]\ny_pred_test_cat = cat_1.predict_proba(df_test_processed)[:, 1]\ny_pred_test_aml = automl.predict(df_test_processed).data[:, 0]\n\n# Combine predictions using weighted average\ny_pred_test_ensemble = (weights[0] * y_pred_test_xgb +\n                        weights[1] * y_pred_test_lgb +\n                        weights[2] * y_pred_test_cat)\n# Create submission file\ndf_sub = pd.read_csv('/kaggle/input/playground-series-s4e10/sample_submission.csv')\ndf_sub['loan_status'] = y_pred_test_ensemble\ndf_sub.to_csv('submission_ensemble.csv', index=False)\ndf_sub.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T00:35:41.466634Z","iopub.status.idle":"2024-10-22T00:35:41.467235Z","shell.execute_reply.started":"2024-10-22T00:35:41.466937Z","shell.execute_reply":"2024-10-22T00:35:41.466972Z"},"trusted":true},"outputs":[],"execution_count":null}]}